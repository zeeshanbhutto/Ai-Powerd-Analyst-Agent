{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25740f7f-bb93-4e6c-8bfe-a24712f0a935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\hadib\\anaconda3\\lib\\site-packages (1.25.3)\n",
      "Requirement already satisfied: langchain in c:\\users\\hadib\\anaconda3\\lib\\site-packages (0.3.18)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain) (0.3.34)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hadib\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain) (2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\hadib\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\hadib\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\hadib\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a392f1-c173-4e9b-9d46-5bb6710f564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a659fadc-2d1c-4e52-867d-26015c9ff8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25eb6a4f-7a6c-4832-8cbe-5aab9e02c7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pymupdf in c:\\users\\hadib\\anaconda3\\lib\\site-packages (1.25.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\hadib\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\hadib\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\hadib\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2cc41ec-ae3a-41ef-8683-b09ac1ca65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using PyMuPDF.\n",
    "    \n",
    "    Parameters:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    \n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    pdf_document = fitz.open(pdf_path)  # Open the PDF file\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)  # Load each page\n",
    "        text += page.get_text()  # Extract text from the page\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3dee114-b85a-4bb8-bc7d-e25f275a2d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted text from Artificial Intelligence Applications in Information and Communication Technologies.pdf\n",
      "‚úÖ Extracted text from generative-ai-and-machine-learning-for-the-enterprise.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Define the extract_text_from_pdf function\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using PyMuPDF.\n",
    "    \n",
    "    Parameters:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    \n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    pdf_document = fitz.open(pdf_path)  # Open the PDF file\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)  # Load each page\n",
    "        text += page.get_text()  # Extract text from the page\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Assuming pdf_folder is the folder where your PDFs are stored\n",
    "pdf_folder = \"data\"\n",
    "pdf_texts = {}  # Dictionary to hold the extracted text\n",
    "\n",
    "# Extract text from all PDFs in the folder\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        \n",
    "        if os.path.exists(pdf_path):  # Extra check for file existence\n",
    "            pdf_texts[filename] = extract_text_from_pdf(pdf_path)\n",
    "            print(f\"‚úÖ Extracted text from {filename}\")\n",
    "        else:\n",
    "            print(f\"‚ùå File not found: {filename}\")\n",
    "\n",
    "# Now you can proceed with generating embeddings, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b1e45f-03ec-4bad-8123-90f3e1fadd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split text for Artificial Intelligence Applications in Information and Communication Technologies.pdf\n",
      "‚úÖ Split text for generative-ai-and-machine-learning-for-the-enterprise.pdf\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Assuming pdf_texts is a dictionary where each value is the text of a PDF\n",
    "pdf_chunks = {}\n",
    "\n",
    "# Split the text of each PDF into chunks\n",
    "for filename, text in pdf_texts.items():\n",
    "    chunks = splitter.split_text(text)\n",
    "    pdf_chunks[filename] = chunks\n",
    "    print(f\"‚úÖ Split text for {filename}\")\n",
    "\n",
    "# Now pdf_chunks will contain the text chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdc852b7-8e4e-4ee4-a87b-4fb00c475d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split Artificial Intelligence Applications in Information and Communication Technologies.pdf into 1220 chunks\n",
      "‚úÖ Split generative-ai-and-machine-learning-for-the-enterprise.pdf into 49 chunks\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "pdf_chunks = {}\n",
    "for filename, text in pdf_texts.items():\n",
    "    pdf_chunks[filename] = splitter.split_text(text)\n",
    "    print(f\"‚úÖ Split {filename} into {len(pdf_chunks[filename])} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62a318f6-bfa4-4cba-8b31-2c76def6d265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted text from Artificial Intelligence Applications in Information and Communication Technologies.pdf\n",
      "‚úÖ Extracted text from generative-ai-and-machine-learning-for-the-enterprise.pdf\n",
      "\n",
      "üìÑ Total PDFs processed: 2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m chunks \u001b[38;5;241m=\u001b[39m pdf_texts  \u001b[38;5;66;03m# Adjust this depending on how you want to chunk\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Get embeddings\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m get_embeddings(chunks)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Create FAISS index\u001b[39;00m\n\u001b[0;32m     31\u001b[0m index \u001b[38;5;241m=\u001b[39m create_faiss_index(embeddings)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pdf_folder = \"data\"  # Ensure this folder exists and contains PDFs\n",
    "pdf_texts = {}  # Dictionary to store extracted text\n",
    "\n",
    "if not os.path.exists(pdf_folder):\n",
    "    print(\"‚ùå Folder not found! Please check the directory path.\")\n",
    "else:\n",
    "    # Loop through all PDFs in the folder\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            \n",
    "            if os.path.exists(pdf_path):  # Extra check for file existence\n",
    "                pdf_texts[filename] = extract_text_from_pdf(pdf_path)\n",
    "                print(f\"‚úÖ Extracted text from {filename}\")\n",
    "            else:\n",
    "                print(f\"‚ùå File not found: {pdf_path}\")\n",
    "\n",
    "# Display total PDFs processed\n",
    "print(f\"\\nüìÑ Total PDFs processed: {len(pdf_texts)}\")\n",
    "\n",
    "\n",
    "# Split the text into chunks (this step depends on how you want to chunk the documents)\n",
    "# For simplicity, let's say each page is a chunk:\n",
    "chunks = pdf_texts  # Adjust this depending on how you want to chunk\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = get_embeddings(chunks)\n",
    "\n",
    "# Create FAISS index\n",
    "index = create_faiss_index(embeddings)\n",
    "\n",
    "# Save the FAISS index\n",
    "save_index(index, \"research_index.faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98f3e4-4552-4314-b5a5-fc6b24eab4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first chunk of the first processed PDF\n",
    "first_pdf = list(pdf_chunks.keys())[0]\n",
    "print(f\"üìÑ First chunk from {first_pdf}:\\n\")\n",
    "print(pdf_chunks[first_pdf][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88016046-37b1-44b0-8fac-930b7c825871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored 6 chunks from generative-ai-and-machine-learning-for-the-enterprise.pdf\n",
      "\n",
      "üîç Most Relevant Results:\n",
      "- be highly curated to business domains and use cases that drive quick gains in productivity and time to value‚Äîsuch as augmenting and automating HR, customer service and code generation. 4. Empowering Y...\n",
      "- Generative AI and ML for the enterprise Contents 01 ‚Üí Introduction 02 ‚Üí Generative AI, foundation models and ML 03 ‚Üí Advantages of foundation models and ML 04 ‚Üí Common use cases and tasks 05 ‚Üí What to...\n",
      "- form of generative AI that combines transformer architecture with unsupervised learning, a ML function. Foundation models are large AI neural networks trained on extensive unlabeled data and fine-tune...\n",
      "- Consulting‚Ñ¢ and the 21,000 data, AI and automation consultants, in addition to the Center of Excellence for Generative AI comprised of more than 1,000 consultants with specialized generative AI expert...\n",
      "- build anomaly detection systems to identify unusual patterns or behaviors in data. Generative AI models can be used to learn the normal data distribution and generate synthetic data samples. The gener...\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for PDF reading\n",
    "import numpy as np\n",
    "import faiss\n",
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    \"\"\"Split text into smaller chunks\"\"\"\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "class PDFEmbeddingStore:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize FAISS index\"\"\"\n",
    "        self.text_chunks = []\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Using a local transformer model\n",
    "        self.dimension = 384  # SentenceTransformer output size\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "\n",
    "    def generate_embedding(self, text):\n",
    "        \"\"\"Generate embedding using SentenceTransformer\"\"\"\n",
    "        return np.array(self.model.encode(text), dtype=np.float32)\n",
    "\n",
    "    def store_pdf(self, pdf_path):\n",
    "        \"\"\"Extracts text, chunks it, generates embeddings, and stores in FAISS\"\"\"\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        chunks = chunk_text(text)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            embedding = self.generate_embedding(chunk).reshape(1, -1)\n",
    "            self.index.add(embedding)\n",
    "            self.text_chunks.append(chunk)\n",
    "        print(f\"‚úÖ Stored {len(chunks)} chunks from {pdf_path}\")\n",
    "\n",
    "    def retrieve_similar(self, query, k=5):\n",
    "        \"\"\"Retrieve top-k similar texts from FAISS\"\"\"\n",
    "        query_embedding = self.generate_embedding(query).reshape(1, -1)\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        return [self.text_chunks[i] for i in indices[0] if i < len(self.text_chunks)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    store = PDFEmbeddingStore()\n",
    "    pdf_path = \"generative-ai-and-machine-learning-for-the-enterprise.pdf\"  # Replace with your PDF file\n",
    "    \n",
    "    store.store_pdf(pdf_path)\n",
    "    query = \"Explain AI in simple words.\"\n",
    "    results = store.retrieve_similar(query)\n",
    "    \n",
    "    print(\"\\nüîç Most Relevant Results:\")\n",
    "    for res in results:\n",
    "        print(\"-\", res[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca69df43-6404-4f4b-b049-df8f2fc73a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored 6 chunks from generative-ai-and-machine-learning-for-the-enterprise.pdf\n",
      "\n",
      "ü§ñ AI Response:\n",
      "Imagine you're teaching a computer to learn by giving it lots of examples of something.  Like, if you want the computer to recognize cats, you show it pictures of cats and dogs, and it learns to spot them based on the features (like pointy ears and fluffy tails). \n",
      "\n",
      "**Artificial intelligence (AI) is like training computers to learn from data and then use that learning to do things like:**\n",
      "\n",
      "* **Understand what we're saying (speech recognition):**  Think about Alexa or Siri!\n",
      "* **Play games like chess (game playing AI):** Computers can now beat human champions at some complex games.\n",
      "* **Recognize faces in photos (facial recognition):** Used for security, social media tagging, and more!\n",
      "\n",
      "AI is about making computers smarter so they can solve problems we face every day, like traffic, weather forecasting, and finding new medicines! \n",
      "\n",
      "\n",
      "Does that make sense? üòä  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for PDF reading\n",
    "import numpy as np\n",
    "import faiss\n",
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    \"\"\"Split text into smaller chunks\"\"\"\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "class PDFEmbeddingStore:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize FAISS index\"\"\"\n",
    "        self.text_chunks = []\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Using a local transformer model\n",
    "        self.dimension = 384  # SentenceTransformer output size\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "\n",
    "    def generate_embedding(self, text):\n",
    "        \"\"\"Generate embedding using SentenceTransformer\"\"\"\n",
    "        return np.array(self.model.encode(text), dtype=np.float32)\n",
    "\n",
    "    def store_pdf(self, pdf_path):\n",
    "        \"\"\"Extracts text, chunks it, generates embeddings, and stores in FAISS\"\"\"\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        chunks = chunk_text(text)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            embedding = self.generate_embedding(chunk).reshape(1, -1)\n",
    "            self.index.add(embedding)\n",
    "            self.text_chunks.append(chunk)\n",
    "        print(f\"‚úÖ Stored {len(chunks)} chunks from {pdf_path}\")\n",
    "\n",
    "    def retrieve_similar(self, query, k=5):\n",
    "        \"\"\"Retrieve top-k similar texts from FAISS\"\"\"\n",
    "        query_embedding = self.generate_embedding(query).reshape(1, -1)\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        return [self.text_chunks[i] for i in indices[0] if i < len(self.text_chunks)]\n",
    "\n",
    "    def generate_response(self, query):\n",
    "        \"\"\"Retrieve relevant content and generate response using Ollama\"\"\"\n",
    "        retrieved_chunks = self.retrieve_similar(query, k=3)\n",
    "        context = \"\\n\".join(retrieved_chunks)\n",
    "\n",
    "        # Format prompt for LLM\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "        \n",
    "        # Generate response using Ollama\n",
    "        response = ollama.generate(model=\"gemma2:2b\", prompt=prompt)\n",
    "        return response['response']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    store = PDFEmbeddingStore()\n",
    "    pdf_path = \"generative-ai-and-machine-learning-for-the-enterprise.pdf\"  # Replace with your PDF file\n",
    "    \n",
    "    store.store_pdf(pdf_path)\n",
    "    query = \"Explain AI in simple words.\"\n",
    "    \n",
    "    response = store.generate_response(query)\n",
    "    \n",
    "    print(\"\\nü§ñ AI Response:\")\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86fb3b-6c97-49ca-ad11-a55d84b53483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
